# Документация заданий по информационному поиску

## Задание 1: Краулер для сбора данных

### Описание
В первом задании реализован простой веб-краулер для сбора HTML-страниц. Краулер скачивает страницы с указанных URL-адресов и сохраняет их локально для дальнейшей обработки.

### Структура и реализация
- Реализован в скрипте `ultra_simple_crawler.py`
- Скачанные страницы сохраняются в директории `data/pages/` в формате `page_N.html`
- Система использует базовые библиотеки для работы с HTTP-запросами
- Индекс скачанных страниц хранится в файле `index.txt`

### Принцип работы
1. Краулер получает список URL-адресов для обработки
2. Для каждого URL выполняет HTTP-запрос и скачивает содержимое страницы
3. Сохраняет HTML-содержимое в файл с уникальным идентификатором
4. Обновляет индекс с информацией о скачанных страницах
5. Реализует базовые механизмы обработки ошибок и ограничения глубины обхода

## Задание 2: Токенизация и лемматизация текстов

### Описание
Второе задание посвящено обработке текста: токенизации (разбиению на термины) и лемматизации (приведению слов к их начальной форме).

### Структура и реализация
- Основной скрипт: `tokenizer.py`
- Результаты хранятся в файлах:
  - `tokens.txt` - список всех токенов (32050 строк)
  - `lemmas.txt` - словарь лемм и их форм (17819 строк)

### Принцип работы
1. Система загружает тексты из HTML-файлов, полученных в Задании 1
2. Выполняет токенизацию текста:
   - Разбивает текст на отдельные слова
   - Удаляет знаки пунктуации и специальные символы
   - Нормализует регистр
3. Для лемматизации используется подход с составлением словаря соответствий:
   - Каждая словоформа приводится к начальной форме (лемме)
   - Создается словарь вида `лемма: список_форм`
4. Результаты сохраняются в текстовые файлы с сохранением связей между леммами и словоформами

## Задание 3: Инвертированный поиск

### Описание
Третье задание включает создание инвертированного индекса для эффективного поиска терминов в коллекции документов.

### Структура и реализация
- Основной скрипт: `main.py` (266 строк)
- Результаты хранятся в файле `inverted_index.json` (1.9MB)

### Принцип работы
1. Система строит инвертированный индекс, который представляет собой структуру вида:
   ```
   {
     "термин1": [id_документа1, id_документа2, ...],
     "термин2": [id_документа3, id_документа5, ...],
     ...
   }
   ```
2. Для каждого токена/термина индекс хранит список идентификаторов документов, в которых этот термин встречается
3. Алгоритм обрабатывает все документы из Задания 1, используя токены из Задания 2
4. Индекс сохраняется в JSON-формате для дальнейшего использования
5. Данная структура позволяет быстро находить все документы, содержащие заданный термин

## Задание 4: Расчет TF-IDF

### Описание
Четвертое задание посвящено вычислению метрик TF-IDF (Term Frequency - Inverse Document Frequency) для терминов и лемм из предыдущих заданий.

### Структура и реализация
- Основной скрипт: `main.py` (188 строк)
- Зависимости указаны в `requirements.txt`:
  - beautifulsoup4 - для извлечения текста из HTML
  - tqdm - для отображения прогресса выполнения
- Результаты сохраняются в директории `results/`:
  - `tokens_tf_idf_<page_id>.txt` - TF-IDF для токенов
  - `lemmas_tf_idf_<page_id>.txt` - TF-IDF для лемм

### Принцип работы
1. Система считывает необходимые данные:
   - Список токенов из Задания 2
   - Словарь лемм из Задания 2
   - Инвертированный индекс из Задания 3
   - HTML-документы из Задания 1

2. Для каждого документа:
   - Извлекается текст из HTML-файла с помощью BeautifulSoup
   - Текст токенизируется (разделяется на слова)
   - Для каждого токена и леммы вычисляется:
     - TF (Term Frequency) - отношение числа вхождений термина к общему количеству терминов в документе
     - IDF (Inverse Document Frequency) - логарифм отношения общего числа документов к числу документов, содержащих термин
     - TF-IDF - произведение TF и IDF

3. Результаты сортируются по убыванию значения TF-IDF и сохраняются в текстовых файлах в формате:
   ```
   <термин/лемма> <idf> <tf-idf>
   ```

4. TF-IDF является важной метрикой, которая позволяет оценить важность термина в контексте конкретного документа из коллекции и используется в задачах информационного поиска и анализа текстов.

### Особенности реализации
- Используется эффективное хранение данных в виде словарей и счетчиков
- Результаты обработки всех 97 HTML-документов хранятся в отдельных файлах
- Система корректно обрабатывает как термины, так и их лемматизированные формы
